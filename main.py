import requests
import feedparser
from datetime import datetime, timedelta
import os
from newspaper import Article, Config

# ==========================================
# 1. ì„¤ì • ë° ì¤€ë¹„
# ==========================================
NOTION_TOKEN = os.environ['NOTION_TOKEN']
DATABASE_ID = os.environ['DATABASE_ID']

headers = {
    "Authorization": "Bearer " + NOTION_TOKEN,
    "Content-Type": "application/json",
    "Notion-Version": "2022-06-28"
}

# ==========================================
# 2. ì²­ì†Œë¶€ (3ì¼ ì§€ë‚œ ë‰´ìŠ¤ ìë™ ì‚­ì œ)
# ==========================================
def delete_old_news():
    print("ğŸ§¹ [ì²­ì†Œ] 3ì¼ ì§€ë‚œ ë‰´ìŠ¤ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤...")
    
    three_days_ago = (datetime.now() - timedelta(days=3)).strftime("%Y-%m-%d")
    
    query_url = f"https://api.notion.com/v1/databases/{DATABASE_ID}/query"
    payload = {
        "filter": {
            "property": "ë‚ ì§œ",
            "date": {"on_or_before": three_days_ago}
        }
    }
    
    response = requests.post(query_url, headers=headers, json=payload)
    results = response.json().get("results", [])

    if not results:
        print("   - ì‚­ì œí•  ì˜¤ë˜ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    for page in results:
        page_id = page["id"]
        requests.patch(f"https://api.notion.com/v1/pages/{page_id}", headers=headers, json={"archived": True})
        print(f"   - ğŸ—‘ï¸ ì‚­ì œë¨ (ID: {page_id})")

# ==========================================
# 3. ë³¸ë¬¸ ì¶”ì¶œ (ì¸ë² ìŠ¤íŒ…ë‹·ì»´ ì°¨ë‹¨ ëŒ€ë¹„)
# ==========================================
def get_full_article(url, summary_fallback=""):
    try:
        config = Config()
        # ë´‡ ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•œ ë¸Œë¼ìš°ì € ìœ„ì¥
        config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.75 Safari/537.36'
        config.request_timeout = 10
        
        article = Article(url, language='ko', config=config)
        article.download()
        article.parse()
        
        # ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìœ¼ë©´(ë³´ì•ˆ ì°¨ë‹¨ë¨) RSS ìš”ì•½ë³¸ìœ¼ë¡œ ëŒ€ì²´
        if len(article.text) < 50:
            if summary_fallback:
                return f"âš ï¸ [ë³´ì•ˆ ì°¨ë‹¨] ê¸°ì‚¬ ë³¸ë¬¸ ìŠ¤í¬ë©ì´ ë§‰í˜€ ìš”ì•½ë³¸ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.\n\n{summary_fallback}"
            else:
                return "ë³¸ë¬¸ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì›ë¬¸ ë§í¬ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
        return article.text
    except:
        if summary_fallback:
            return f"âš ï¸ [ì ‘ì† ì—ëŸ¬] ë³¸ë¬¸ ëŒ€ì‹  ìš”ì•½ë³¸ì„ ë³´ì—¬ë“œë¦½ë‹ˆë‹¤.\n\n{summary_fallback}"
        return "ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨"

# ==========================================
# 4. ë…¸ì…˜ ì—…ë¡œë“œ (ì¶œì²˜ í‘œì‹œ ê¸°ëŠ¥)
# ==========================================
def create_page(category, source_name, title, link, date, content):
    url = "https://api.notion.com/v1/pages"
    
    # ì œëª© ì•ë¨¸ë¦¬ì— [ì¶œì²˜] ë¶™ì´ê¸°
    final_title = f"[{source_name}] {title}"

    # ë‚´ìš© ê¸¸ë©´ ìë¥´ê¸°
    if len(content) > 1800:
        content = content[:1800] + "\n...(ì¤‘ëµ)... (ì „ì²´ ë‚´ìš©ì€ ì•„ë˜ ë§í¬ í™•ì¸)"

    data = {
        "parent": {"database_id": DATABASE_ID},
        "properties": {
            "ì´ë¦„": {"title": [{"text": {"content": final_title}}]},
            "URL": {"url": link},
            "ë‚ ì§œ": {"date": {"start": date}},
            "ì¹´í…Œê³ ë¦¬": {"select": {"name": category}}
        },
        "children": [
            {
                "object": "block",
                "type": "callout",
                "callout": {
                    "rich_text": [{"text": {"content": "ê¸°ì‚¬ ë‚´ìš©"}}],
                    "icon": {"emoji": "ğŸ“°"},
                    "color": "gray_background"
                }
            },
            {
                "object": "block",
                "type": "paragraph",
                "paragraph": {
                    "rich_text": [{"text": {"content": content}}]
                }
            },
            {
                "object": "block",
                "type": "paragraph",
                "paragraph": {
                    "rich_text": [{"text": {"content": "ğŸ‘‰ ì›ë¬¸ ì „ì²´ ë³´ëŸ¬ê°€ê¸°: " + link, "link": {"url": link}}}]
                }
            }
        ]
    }
    requests.post(url, headers=headers, json=data)

# ==========================================
# 5. ë©”ì¸ ì‹¤í–‰ (5ëŒ€ì¥ ë‰´ìŠ¤ ì†ŒìŠ¤)
# ==========================================
delete_old_news()

print("\nğŸ“° [ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘] 5ê°œ ì–¸ë¡ ì‚¬ì—ì„œ í•µì‹¬ ê¸°ì‚¬ 4ê°œì”© ê°€ì ¸ì˜µë‹ˆë‹¤...")

targets = [
    # --- 1. ë¯¸êµ­ ì£¼ì‹ (í•œê²½ê¸€ë¡œë²Œ + ì¸ë² ìŠ¤íŒ…) ---
    {
        "category": "ë¯¸êµ­ì£¼ì‹",
        "source": "í•œê²½ê¸€ë¡œë²Œ",
        "rss": "https://rss.hankyung.com/feed/international"
    },
    {
        "category": "ë¯¸êµ­ì£¼ì‹",
        "source": "ì¸ë² ìŠ¤íŒ…",
        "rss": "https://kr.investing.com/rss/news_285.rss" # ì¸ë² ìŠ¤íŒ… ì£¼ì‹ì‹œì¥ ë‰´ìŠ¤
    },
    
    # --- 2. êµ­ë‚´ ì£¼ì‹ (í•œêµ­ê²½ì œ + ë§¤ì¼ê²½ì œ) ---
    {
        "category": "êµ­ë‚´ì£¼ì‹",
        "source": "í•œêµ­ê²½ì œ",
        "rss": "https://rss.hankyung.com/feed/stock"
    },
    {
        "category": "êµ­ë‚´ì£¼ì‹",
        "source": "ë§¤ì¼ê²½ì œ",
        "rss": "https://www.mk.co.kr/rss/50200011/"
    },

    # --- 3. ì½”ì¸ (ì½”ì¸ë°ìŠ¤í¬/í† í°í¬ìŠ¤íŠ¸) ---
    {
        "category": "ì½”ì¸",
        "source": "ì½”ì¸ë°ìŠ¤í¬", # ì‹¤ì œ ë°ì´í„°ëŠ” ì•ˆì •ì ì¸ í† í°í¬ìŠ¤íŠ¸ RSS ì‚¬ìš©
        "rss": "https://www.tokenpost.kr/rss"
    }
]

for target in targets:
    category = target["category"]
    source = target["source"]
    rss_url = target["rss"]
    
    print(f"\nğŸ” [{category} - {source}] ë‰´ìŠ¤ 4ê°œ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
    
    try:
        feed = feedparser.parse(rss_url)
        
        count = 0
        MAX_ARTICLES = 4 # ì–¸ë¡ ì‚¬ë³„ë¡œ ê°€ì ¸ì˜¬ ê¸°ì‚¬ ê°œìˆ˜ (ì›í•˜ì‹œë©´ ìˆ«ìë¥¼ 5ë‚˜ 6ìœ¼ë¡œ ëŠ˜ë ¤ë„ ë©ë‹ˆë‹¤)
        
        for entry in feed.entries:
            if count >= MAX_ARTICLES:
                break
                
            # ë‚ ì§œ ì²˜ë¦¬
            if hasattr(entry, 'published_parsed'):
                dt = datetime(*entry.published_parsed[:6]).isoformat()
            else:
                dt = datetime.now().isoformat()
            
            # ì¸ë² ìŠ¤íŒ…ë‹·ì»´ ë“± ëŒ€ë¹„ìš© ìš”ì•½ë³¸ ì¤€ë¹„
            summary_fallback = entry.get('description', '')
            summary_fallback = summary_fallback.replace('<p>', '').replace('</p>', '').replace('<br>', '\n')

            # ë³¸ë¬¸ ì¶”ì¶œ
            full_text = get_full_article(entry.link, summary_fallback)
            
            # ë…¸ì…˜ ì—…ë¡œë“œ
            create_page(category, source, entry.title, entry.link, dt, full_text)
            print(f"   âœ… ì €ì¥ ì™„ë£Œ: [{source}] {entry.title}")
            
            count += 1

    except Exception as e:
        print(f"   âŒ {source} ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {e}")

print("\nğŸ‰ ëª¨ë“  ë‰´ìŠ¤ ë°°ë‹¬ ì™„ë£Œ!")
